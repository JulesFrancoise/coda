import { parseParameters } from '@coda/prelude';
import { disposeBoth } from '@most/disposable';
import XmmPredictionSink from '../core/xmm_prediction_sink';

/**
 * Parameter definitions
 * @ignore
 */
const definitions = type => ({
  model: {
    required: true,
    type: 'any',
    default: null,
  },
  likelihoodWindow: {
    required: false,
    type: 'integer',
    default: 1,
    min: 1,
  },
  output: {
    type: 'enum',
    default: 'smoothedNormalizedLikelihoods',
    list: [
      'all',
      'instantLikelihoods',
      'instantNormalizedLikelihoods',
      'smoothedLikelihoods',
      'smoothedLogLikelihoods',
      'smoothedNormalizedLikelihoods',
      'likeliest',
      'outputValues',
    ].concat((type === 'gmm')
      ? ['beta']
      : ['alpha', 'progress']),
  },
});

/**
 * XMM Recognition operator factory
 * @private
 * @param  {String} [type] Model type ('gmm', 'hmm', 'hhmm').
 * @return {Function}      Recognition function
 */
const xmmPredictFactory = type =>
  function xmmPredict(options = {}, source) {
    if (!options.model || !options.model.run) {
      throw new Error('The `xmmPredicy` module requires a stream of model parameters');
    }
    const { model } = options;
    if (!model.attr.type || model.attr.type !== type) {
      throw new Error(`The 'xmmPredicy' module requires a model stream of type '${type}'`);
    }
    const params = parseParameters(definitions(type), options);
    let fetchOutput;
    if (params.output === 'all') {
      fetchOutput = res => Object.assign({}, res);
    } else if (['alpha', 'beta'].includes(params.output)) {
      fetchOutput = (res, pred) => pred.models[res.likeliest][params.output].slice();
    } else {
      fetchOutput = res => res[params.output].slice();
    }
    return {
      attr: {
        format: 'vector',
        size: (params.output === 'outputValues') ? 2 : 1,
        varsize: true,
      },
      run(sink, scheduler) {
        const predictionSink = new XmmPredictionSink(
          model,
          fetchOutput,
          params.likelihoodWindow,
          sink,
          scheduler,
        );
        const modelDisposable = model.run(predictionSink.modelStream, scheduler);
        const dataDisposable = source.run(predictionSink, scheduler);

        return disposeBoth(modelDisposable, dataDisposable);
      },
    };
  };

/**
 * Real-time recognition using GMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link gmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link gmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.gmmTrain({ gaussians: 3 });
 *
 * // Perform real-time recognition
 * c = a.gmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'GMM-based recognition' });
 */
export const gmmPredict = xmmPredictFactory('gmm');

/**
 * Real-time recognition using HMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link hmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link hmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.hmmTrain({ states: 5 });
 *
 * // Perform real-time recognition
 * c = a.hmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'HMM-based recognition' });
 */
export const hmmPredict = xmmPredictFactory('hmm');

/**
 * Real-time recognition using Hierarchical HMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link hhmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link hhmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.hhmmTrain({ states: 5 });
 *
 * // Perform real-time recognition
 * c = a.hhmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'HMM-based recognition' });
 */
export const hhmmPredict = xmmPredictFactory('hhmm');
