import { parseParameters } from '@coda/prelude';
import { disposeBoth } from '@most/disposable';
import XmmPredictionSink from '../core/xmm_prediction_sink';

/**
 * Parameter definitions
 * @ignore
 */
const definitions = type => ({
  model: {
    required: true,
    type: 'any',
    default: null,
  },
  likelihoodWindow: {
    required: false,
    type: 'integer',
    default: 1,
    min: 1,
  },
  output: {
    type: 'enum',
    default: 'smoothedNormalizedLikelihoods',
    list: [
      'all',
      'instantLikelihoods',
      'instantNormalizedLikelihoods',
      'smoothedLikelihoods',
      'smoothedLogLikelihoods',
      'smoothedNormalizedLikelihoods',
      'likeliest',
      'outputValues',
    ].concat((type === 'gmm')
      ? ['beta']
      : ['alpha', 'progress']),
  },
});

/**
 * XMM Recognition operator factory
 * @private
 * @param  {String} [type] Model type ('gmm', 'hmm', 'hhmm').
 * @return {Function}      Recognition function
 */
const xmmPredictFactory = type => function xmmPredict(options = {}, source) {
  if (!options.model || !options.model.run) {
    throw new Error('The `xmmPredicy` module requires a stream of model parameters');
  }
  const { model } = options;
  if (!model.attr.type || model.attr.type !== type) {
    throw new Error(`The 'xmmPredicy' module requires a model stream of type '${type}'`);
  }
  const params = parseParameters(definitions(type), options);
  let fetchOutput;
  if (params.output === 'all') {
    fetchOutput = res => Object.assign({}, res);
  } else if (['alpha', 'beta'].includes(params.output)) {
    fetchOutput = (res, pred) => pred.models[res.likeliest][params.output].slice();
  } else {
    fetchOutput = res => res[params.output].slice();
  }
  return {
    attr: {
      format: 'vector',
      size: (params.output === 'outputValues') ? 2 : 1,
      varsize: true,
    },
    run(sink, scheduler) {
      const predictionSink = new XmmPredictionSink(
        model,
        fetchOutput,
        params.likelihoodWindow,
        sink,
        scheduler,
      );
      const modelDisposable = model.run(predictionSink.modelStream, scheduler);
      const dataDisposable = source.run(predictionSink, scheduler);

      return disposeBoth(modelDisposable, dataDisposable);
    },
  };
};

/**
 * Real-time recognition using GMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link gmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link gmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.gmmTrain({ gaussians: 3 });
 *
 * // Perform real-time recognition
 * c = a.gmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'GMM-based recognition' });
 */
export const gmmPredict = xmmPredictFactory('gmm');

/**
 * Real-time recognition using HMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link hmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link hmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.hmmTrain({ states: 5 });
 *
 * // Perform real-time recognition
 * c = a.hmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'HMM-based recognition' });
 */
export const hmmPredict = xmmPredictFactory('hmm');

/**
 * Real-time recognition using Hierarchical HMMs
 *
 * The operator applies to the same data stream as the one used for training, and the model
 * parameters should be passed as a stream (`model` argument) generated by the {@link hhmmTrain}
 * operator.
 *
 * @todo More details here...
 *
 * @param  {Object} [options={}] Recognizer options
 * @param  {Stream} [options.model=null] Stream of model parameters obtained through
 * {@link hhmmTrain}.
 * @param  {string} [options.likelihoodWindow=1] Size of the likelihood smoothing window
 * @param  {string} [options.output='smoothedLogLikelihoods'] Type of output data.
 * @param  {Stream} source input data stream
 * @return {Stream}
 *
 * @example
 * // Generate a smooth random signal
 * a = periodic(20)
 * .rand({ size: 2 })
 * .biquad({ f0: 2 })
 * .plot({ legend: 'Data Stream' });
 *
 * // Setup a data recorder
 * b = a.recorder({ name: 'data' });
 *
 * // Dynamically train when changes occur in the recorder
 * model = b.hhmmTrain({ states: 5 });
 *
 * // Perform real-time recognition
 * c = a.hhmmPredict({ model })
 *   .plot({ fill: 'bottom', stacked: true, legend: 'HMM-based recognition' });
 */
export const hhmmPredict = xmmPredictFactory('hhmm');
